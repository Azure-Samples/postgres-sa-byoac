# 5.4 チャットエンドポイントを有効にする

このステップでは、`completions` ルーター内の `/chat` エンドポイントのバックエンドAPIコードを確認します。その後、`completions` ルーターをFastAPIアプリケーションに追加し、`/chat` エンドポイントを利用可能にします。

## チャットエンドポイントの実装を確認する

_Woodgove Bank API_ は、さまざまな _ルーター_ でエンドポイントを公開しています... `chat` エンドポイントは、`src/api/app/routers/completions.py` ファイルで定義されている `completions` ルーターにあります。今すぐVS Codeで開き、コードをセクションごとに探索してください。また、以下のセクションを展開して、インラインでコードを確認し、各コード行の説明を確認することもできます。

???+ info "チャットエンドポイントコード"

    ```python linenums="1" title="src/api/app/routers/completions.py"
    from app.functions.chat_functions import ChatFunctions
    from app.lifespan_manager import get_chat_client, get_db_connection_pool, get_embedding_client, get_prompt_service
    from app.models import CompletionRequest
    from fastapi import APIRouter, Depends
    from langchain.agents import AgentExecutor, create_openai_functions_agent
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain_core.tools import StructuredTool
    
    # Initialize the router
    router = APIRouter(
        prefix = "/completions",
        tags = ["Completions"],
        dependencies = [Depends(get_chat_client)],
        responses = {404: {"description": "Not found"}}
    )
    
    @router.post('/chat', response_model = str)
    async def generate_chat_completion(
        request: CompletionRequest,
        llm = Depends(get_chat_client),
        db_pool = Depends(get_db_connection_pool),
        embedding_client = Depends(get_embedding_client),
        prompt_service = Depends(get_prompt_service)):
        """Generate a chat completion using the Azure OpenAI API."""
            
        # Retrieve the copilot prompt
        system_prompt = prompt_service.get_prompt("copilot")
    
        # Provide the copilot with a persona using the system prompt.
        messages = [{ "role": "system", "content": system_prompt }]
    
        # Add the chat history to the messages list
        # Chat history provides context of previous questions and responses for the copilot.
        for message in request.chat_history[-request.max_history:]:
            messages.append({"role": message.role, "content": message.content})
    
        # Create a chat prompt template
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder("chat_history", optional=True),
                ("user", "{input}"),
                MessagesPlaceholder("agent_scratchpad")
            ]
        )
    
        # Get the chat functions
        cf = ChatFunctions(db_pool, embedding_client)
    
        # Define tools for the agent to retrieve data from the database
        tools = [
            # Hybrid search functions
            StructuredTool.from_function(coroutine=cf.find_invoice_line_items),
            StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),
            StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),
            StructuredTool.from_function(coroutine=cf.find_sow_chunks),
            StructuredTool.from_function(coroutine=cf.find_sow_validation_results),
            # Get invoice data functions
            StructuredTool.from_function(coroutine=cf.get_invoice_id),
            StructuredTool.from_function(coroutine=cf.get_invoice_line_items),
            StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),
            StructuredTool.from_function(coroutine=cf.get_invoices),
            # Get SOW data functions
            StructuredTool.from_function(coroutine=cf.get_sow_chunks),
            StructuredTool.from_function(coroutine=cf.get_sow_id),
            StructuredTool.from_function(coroutine=cf.get_sow_milestones),
            StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),
            StructuredTool.from_function(coroutine=cf.get_sow_validation_results),
            StructuredTool.from_function(coroutine=cf.get_sows),
            # Get vendor data functions
            StructuredTool.from_function(coroutine=cf.get_vendors)
        ]
        
        # Create an agent
        agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)
        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)
        completion = await agent_executor.ainvoke({"input": request.message, "chat_history": messages})
        return completion['output']
    ```

1. **ライブラリのインポート** (行1-7): 必要なクラスと関数がさまざまなライブラリからインポートされます。

2. **ルーターの初期化** (行10-15): これは `completions` ルーターで、ルートの _プレフィックス_、依存関係、およびその他のメタデータを割り当てます。

3. **チャットエンドポイントの定義** (行17-23): `/chat` エンドポイントは、_Woodgove Bank_ コパイロット実装へのエントリーポイントです。ユーザーのクエリ、チャット履歴、およびプロンプトに含める履歴メッセージの最大数を含む `CompletionRequest` を期待し、テキスト応答を返します。

      - クライアントからのPOSTリクエストを受け入れ、必要なパラメータを抽出します。
      - これらのパラメータを使用して _get_chat_completion_ 関数を呼び出します。
      - LLMの応答をクライアントに返します。

4. **チャットエンドポイントの実装** (行24-78): "/completions/chat" ルートは、Contoso Chat実装を呼び出すエンドポイントにマッピングされます。

      - **システムプロンプトの取得** (行27): システムプロンプトは、コパイロットの _ペルソナ_ を定義し、コパイロットがどのように振る舞い、質問に答え、顧客と対話するべきかについての指示を提供します。また、RAGデザインパターンについてのガイダンスや、質問に答える際に関数呼び出し（ツール）がどのように使用されるべきかについても提供します。この詳細については、このセクションの [プロンプトエンジニアリングのステップ](./05.md) で詳しく見ていきます。

- **メッセージコレクションの構築** (行 30-35): メッセージコレクションは、LLMにシステムおよびユーザープロンプトとチャット履歴メッセージを提供します。各メッセージは`role`とメッセージ`content`で構成されます。`role`は`system`、`assistant`、または`user`になります。`system`メッセージの後、すべての後続メッセージは`user` / `assistant`ペアでなければならず、ユーザーのクエリに続いてアシスタントの応答が続きます。

- **モデルプロンプトの構築** (行 38-45): LangChainの`ChatPromptTemplate`クラスを使用すると、メッセージのコレクションから_モデルプロンプト_を構築できます。
  - _システムプロンプト_は、モデルに指示を提供するために追加されます。
  - _チャット履歴_は、以前の質問と応答に関するコンテキストとして挿入されます。
  - _ユーザー入力_は、モデルが回答しようとしている現在の質問を提供します。
  - _エージェントスクラッチパッド_プレースホルダーは、エージェントに割り当てられたツールからの応答を許可し、モデルを基礎データで_補強_します。
  - 結果として得られるプロンプトは、会話型AIエージェントに構造化された入力を提供し、与えられたコンテキストに基づいて応答を生成するのに役立ちます。

- **関数呼び出しの実装** (行 48-72):
  - 行 48では、PostgreSQLデータベースと対話するためのメソッドを含む`ChatFunctions`クラスをインスタンス化します。`src/api/app/functions/chat_functions.py`ファイルで関数を確認できます。
  - 行 51-72で作成された`tools`配列は、LangChainエージェントが応答_生成_中にモデルプロンプトを_補強_するための_取得_操作を実行するために利用可能な_関数_のコレクションです。
  - ツールは、LangChainが提供する`StructuredTool.from_function`メソッドを使用して作成されます。

    ??? info "LangChain `StructuredTool`クラスについて"

        `StructuredTool`クラスは、LangChainエージェントが関数と対話するためのラッパーです。`from_function`メソッドは、入力パラメータとドックストリングの説明を使用して関数を記述し、指定された関数からツールを作成します。非同期メソッドで使用するには、関数名を`coroutine`入力パラメータに渡します。

    
                Pythonでは、ドキュメンテーション文字列（docstring）は、関数、メソッド、クラス、またはモジュールを文書化するために使用される特別な種類の文字列です。Pythonコードにドキュメントを関連付ける便利な方法を提供し、通常は三重引用符（""" または '''）で囲まれています。ドキュメンテーション文字列は、文書化する関数（またはメソッド、クラス、モジュール）の定義直後に配置されます。

                `StructuredTool.from_function` メソッドを使用すると、Azure OpenAIの関数呼び出しメソッドで必要なJSON関数定義の作成が自動化され、LangChainを使用する際の関数呼び出しが簡素化されます。

      - **LangChainエージェントを作成する**（行75-76）：LangChainエージェントは、LLMと対話して応答を生成する役割を担います。

         - `create_openai_functions_agent` メソッドを使用して、LangChainエージェントがインスタンス化されます。このエージェントは、エージェントに提供された `tools` を介して_関数呼び出し_を処理します。

            ??? info "`create_openai_functions_agent` 関数について"

                LangChainの `create_openai_functions_agent` 関数は、指定された言語モデルとツールを使用してタスクを実行するために外部関数を呼び出すことができるエージェントを作成します。これにより、エージェントのワークフローにさまざまなサービスや機能を統合し、柔軟性と拡張された機能を提供します。

         - LangChainの `AgentExecutor` クラスは、エージェントの実行フローを管理します。入力の処理、ツールやモデルの呼び出し、出力の処理を担当します。

            ??? info "LangChainの `AgentExecutor` について"

                `AgentExecutor` は、応答を生成するために必要なすべてのステップが正しい順序で実行されることを保証します。エージェントの実行の複雑さを抽象化し、追加の機能と構造を提供し、洗練されたエージェントを構築、管理、スケールしやすくします。

      - **エージェントを呼び出す**（行77）：エージェントエグゼキューターの非同期 `invoke` メソッドは、受信したユーザーメッセージとチャット履歴をLLMに送信します。

- `input` と `chat_history` トークンは、`ChatPromptTemplate` を使用して作成されたプロンプトオブジェクトで定義されました。`invoke` メソッドはこれらをモデルプロンプトに注入し、LLM が応答を生成する際にその情報を使用できるようにします。
- LangChain エージェントは、_ユーザーのクエリ_ を評価することで、ツール呼び出しが必要かどうかを判断するために LLM を使用します。
- 質問に答えるために必要な _ツール_ は呼び出され、その結果から得られる基礎データでモデルプロンプトが強化され、最終的な応答が形成されます。

- **応答を返す** (78行目): エージェントの完了応答がユーザーに返されます。

## チャットエンドポイント呼び出しを有効にする

_Woodgrove Bank Contract Management Portal_ から `/chat` エンドポイントを呼び出せるようにするために、FastAPI アプリに completions ルーターを追加します。

1. VS Code の **エクスプローラー** で、`src/api/app` フォルダーに移動し、`main.py` ファイルを開きます。

2. API エンドポイントルーターが追加されているコードブロックを見つけます (44-56行目)。

3. そのブロックの開始部分 (43行目の `# Add routers to API endpoints` コメントの直下) に以下のコードを挿入し、`completions/chat` エンドポイントを公開 API に追加します。

    !!! danger "以下のコードを `main.py` の 43行目に挿入してください！"

    ```python
    app.include_router(completions.router)
    ```

4. 更新されたルーターのリストは次のようになります。

    ```python
    # Add routers to API endpoints
    app.include_router(completions.router)
    app.include_router(deliverables.router)
    app.include_router(documents.router)
    app.include_router(embeddings.router)
    app.include_router(invoices.router)
    app.include_router(invoice_line_items.router)
    app.include_router(milestones.router)
    app.include_router(sows.router)
    app.include_router(status.router)
    app.include_router(statuses.router)
    app.include_router(validation.router)
    app.include_router(validation_results.router)
    app.include_router(vendors.router)
    app.include_router(webhooks.router)
    ```

5. `main.py` ファイルを保存します。

---

!!! success "おめでとうございます！API はコパイロットとの対話が可能になりました！"
